!pip install transformers tensorflow

from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")

"""## Summarization with Few-Shot Learning"""

# Here, we provide a few examples along with their summaries to help the model understand the pattern.

# Few-shot examples
few_shot_examples = [
    "Summarise: 'The quick brown fox jumps over the lazy dog. The dog was not amused by the fox's antics.' Summary: 'The fox jumped over the dog, who was not happy.'",
    "Summarise: 'The rain in Spain stays mainly in the plain. It was a wet and rainy season in the Spanish plains.' Summary: 'It was a rainy season in the Spanish plains.'"
]

# New prompt to summarize
prompt = "Summarise: 'Studies show that eating carrots helps improve vision. Carrots contain beta-carotene, a substance that the body converts into vitamin A, crucial for maintaining healthy eyesight.'"

# Combine examples and prompt
combined_prompt = "\n\n".join(few_shot_examples + [prompt])

# Generate inputs
inputs = tokenizer(combined_prompt, return_tensors="tf", max_length=512, truncation=True, padding=True)

# Generate outputs
outputs = model.generate(inputs["input_ids"], max_length=150, num_beams=5, early_stopping=True)

# Decode and print the summary
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""## Translation with Few-Shot Learning"""

# Here, we provide a few examples along with their translations to help the model understand the pattern.

# Few-shot examples
few_shot_examples = [
    "translate English to Spanish: 'The cat sits on the mat.' Translation: 'El gato se sienta en la alfombra.'",
    "translate English to Spanish: 'The sun is shining brightly.' Translation: 'El sol brilla intensamente.'"
]

# New prompt to translate
translation_prompt = "translate English to Spanish: 'Cheese is delicious.'"

# Combine examples and prompt
combined_translation_prompt = "\n\n".join(few_shot_examples + [translation_prompt])

# Prepare inputs and generate outputs
translation_inputs = tokenizer(combined_translation_prompt, return_tensors="tf", max_length=512, truncation=True, padding=True)
translation_outputs = model.generate(translation_inputs["input_ids"], max_length=40, num_beams=5, early_stopping=True)

# Decode and display the translation
print(tokenizer.decode(translation_outputs[0], skip_special_tokens=True))

"""## Q&A with In-Context Learning"""

# Here, we provide a few examples along with their answers to help the model understand the pattern.

# Few-shot examples
few_shot_examples = [
    "The Great Wall of China is over 13,000 miles long. question: 'How long is the Great Wall of China?' answer: 'The Great Wall of China is over 13,000 miles long.'",
    "The capital of France is Paris. question: 'What is the capital of France?' answer: 'The capital of France is Paris.'"
]

# New context and question
context_question = "Mount Everest is the highest mountain in the world. question: 'What is the highest mountain in the world?'"

# Combine examples and prompt
combined_qa_prompt = "\n\n".join(few_shot_examples + [context_question])

# Generate inputs
question_inputs = tokenizer(combined_qa_prompt, return_tensors="tf", max_length=512, truncation=True, padding=True)

# Generate outputs
question_outputs = model.generate(question_inputs["input_ids"], max_length=50, num_beams=5, early_stopping=True)

# Decode and print the answer
print(tokenizer.decode(question_outputs[0], skip_special_tokens=True))

